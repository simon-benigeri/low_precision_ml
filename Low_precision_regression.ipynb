{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 Low Precision Training Example\n",
    "In this notebook, we present a quick example of how to simulate training a deep neural network in low precision with QPyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from qtorch.quant import Quantizer, quantizer\n",
    "from qtorch.optim import OptimLP\n",
    "from torch.optim import SGD\n",
    "from qtorch import FloatingPoint, FixedPoint\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston, load_diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the data. In this example, we will experiment with regression datasets, `boston` and `diabetes` from `scikit-learn.datasets`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.y = y\n",
    "        self.X = X\n",
    "        \n",
    "        # we mide need some transforms like sklearn scalers\n",
    "        # self.transform = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        target = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        # features = self.X[idx]\n",
    "        # target = self.y[idx]\n",
    "        \"\"\"\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "            target = self.transform(target)\n",
    "        \"\"\"\n",
    "        \n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLP(nn.Module):\n",
    "    \"\"\"\n",
    "    a low precision Logistic Regression model\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int=5, quant: Quantizer=None):\n",
    "        super(LinearLP, self).__init__()\n",
    "        self.W = nn.Linear(input_size, 1)\n",
    "        self.quant = quant\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.W(x)\n",
    "        if self.quant:\n",
    "            out = self.quant(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = 'boston'\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "if DATA == 'boston':\n",
    "    data = load_boston()\n",
    "elif DATA == 'diabetes':\n",
    "    data = load_diabetes()\n",
    "\n",
    "X, y = data['data'], data['target']\n",
    "\n",
    "INPUT_SIZE = X[0].shape[0]\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create custom datasets with transforms\n",
    "training_data = RegressionDataset(X_train, y_train)\n",
    "test_data = RegressionDataset(X_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the quantization setting we are going to use. In particular, here we follow the setting reported in the paper \"Training Deep Neural Networks with 8-bit Floating Point Numbers\", where the authors propose to use specialized 8-bit and 16-bit floating point format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_16 = FloatingPoint(exp=6, man=9)\n",
    "Q = Quantizer(forward_number=bit_16, backward_number=bit_16,\n",
    "              forward_rounding=\"nearest\", backward_rounding=\"nearest\")\n",
    "\n",
    "model = LinearLP(input_size=INPUT_SIZE, quant=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a low-precision ResNet. In the definition, we recursively insert quantization module after every convolution layer. Note that the quantization of weight, gradient, momentum, and gradient accumulator are not handled here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the low-precision optimizer wrapper to help define the quantization of weight, gradient, momentum, and gradient accumulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reuse common training scripts without any extra codes to handle quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, model, criterion, optimizer=None, phase=\"train\"):\n",
    "    assert phase in [\"train\", \"eval\"], \"invalid running phase\"\n",
    "    loss_sum = 0.0\n",
    "    correct = 0.0\n",
    "\n",
    "    if phase==\"train\": model.train()\n",
    "    elif phase==\"eval\": model.eval()\n",
    "\n",
    "    ttl = 0\n",
    "    with torch.autograd.set_grad_enabled(phase==\"train\"):\n",
    "        for i, (input, target) in tqdm(enumerate(loader), total=len(loader)):\n",
    "            input = input.to(device=device)\n",
    "            target = target.to(device=device)\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            loss_sum += loss.cpu().item() * input.size(0)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            ttl += input.size()[0]\n",
    "\n",
    "            if phase==\"train\":\n",
    "                loss = loss * 1000 # do loss scaling\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    correct = correct.cpu().item()\n",
    "    return {\n",
    "        'loss': loss_sum / float(ttl),\n",
    "        'accuracy': correct / float(ttl) * 100.0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin the training process just as usual. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]<ipython-input-46-a1816dccc0aa>:15: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = criterion(output, target)\n",
      "<ipython-input-46-a1816dccc0aa>:15: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = criterion(output, target)\n",
      "100%|██████████| 12/12 [00:00<00:00, 498.73it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]<ipython-input-46-a1816dccc0aa>:15: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = criterion(output, target)\n",
      "100%|██████████| 5/5 [00:00<00:00, 952.95it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 730.74it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 933.15it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1034.40it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 897.37it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1339.28it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 1460.72it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1555.80it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 1501.94it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1277.52it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 1488.08it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1419.91it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 1510.59it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1522.25it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 1104.05it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1531.00it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 1551.38it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1552.39it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 1549.89it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    train_res = run_epoch(train_dataloader, model, F.mse_loss,\n",
    "                                optimizer=optimizer, phase=\"train\")\n",
    "    test_res = run_epoch(test_dataloader, model, F.mse_loss,\n",
    "                                optimizer=optimizer, phase=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': nan, 'accuracy': 0.0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': nan, 'accuracy': 0.0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define number formats used in forward and backward quantization\n",
    "from qtorch import FixedPoint, FloatingPoint\n",
    "# Create a quantizer\n",
    "from qtorch.quant import Quantizer\n",
    "\n",
    "# forward_num = FixedPoint(wl=4, fl=2)\n",
    "# backward_num = FloatingPoint(exp=5, man=2)\n",
    "bit_8 = FloatingPoint(exp=5, man=2)\n",
    "bit_16 = FloatingPoint(exp=6, man=9)\n",
    "\n",
    "# define quantization functions\n",
    "weight_quant = quantizer(forward_number=bit_16,\n",
    "                        forward_rounding=\"nearest\")\n",
    "grad_quant = quantizer(forward_number=bit_16,\n",
    "                        forward_rounding=\"nearest\")\n",
    "momentum_quant = quantizer(forward_number=bit_16,\n",
    "                        forward_rounding=\"nearest\")\n",
    "acc_quant = quantizer(forward_number=bit_16,\n",
    "                        forward_rounding=\"nearest\")\n",
    "\n",
    "# define a lambda function so that the Quantizer module can be duplicated easily\n",
    "act_error_quant = lambda : Quantizer(forward_number=bit_16, backward_number=bit_16,\n",
    "                        forward_rounding=\"nearest\", backward_rounding=\"nearest\")\n",
    "\n",
    "\n",
    "Q = Quantizer(forward_number=bit_16, backward_number=bit_16,\n",
    "              forward_rounding=\"nearest\", backward_rounding=\"stochastic\")\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "Q_optimizer = OptimLP(optimizer,\n",
    "                    weight_quant=weight_quant,\n",
    "                    grad_quant=grad_quant,\n",
    "                    momentum_quant=momentum_quant,\n",
    "                    acc_quant=acc_quant,\n",
    "                    grad_scaling=1/1000 # do loss scaling\n",
    ")\n",
    "\n",
    "model = PreResNet(act_error_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 1.5749474658966065, 'accuracy': 43.63}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
