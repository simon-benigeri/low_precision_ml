{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install qtorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williamansehl/opt/anaconda3/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import useful modules\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from qtorch.quant import Quantizer, quantizer\n",
    "from qtorch.optim import OptimLP\n",
    "from torch.optim import SGD\n",
    "from qtorch import FloatingPoint\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "ds = torchvision.datasets.CIFAR10\n",
    "path = os.path.join(\"./data\", \"CIFAR10\")\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "train_set = ds(path, train=True, download=True, transform=transform_train)\n",
    "test_set = ds(path, train=False, download=True, transform=transform_test)\n",
    "loaders = {\n",
    "        'train': torch.utils.data.DataLoader(\n",
    "            train_set,\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        ),\n",
    "        'test': torch.utils.data.DataLoader(\n",
    "            test_set,\n",
    "            batch_size=128,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, quant, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.quant = quant()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.quant(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.quant(out)\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.quant(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.quant(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "\n",
    "        return out\n",
    "    \n",
    "class PreResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,quant, num_classes=10, depth=20, method='nearest'):\n",
    "\n",
    "        super(PreResNet, self).__init__()\n",
    "        assert (depth - 2) % 6 == 0, 'depth should be 6n+2'\n",
    "        n = (depth - 2) // 6\n",
    "\n",
    "        block = BasicBlock\n",
    "\n",
    "        self.inplanes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.layer1 = self._make_layer(block, 16, n, quant)\n",
    "        self.layer2 = self._make_layer(block, 32, n, quant, stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, n, quant, stride=2)\n",
    "        self.bn = nn.BatchNorm2d(64 * block.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "        self.quant = quant()\n",
    "        IBM_half = FloatingPoint(exp=6, man=9)\n",
    "        self.quant_half = Quantizer(IBM_half, IBM_half, method, method)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, quant, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "            )\n",
    "\n",
    "        layers = list()\n",
    "        layers.append(block(self.inplanes, planes, quant , stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, quant))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant_half(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.quant(x)\n",
    "\n",
    "        x = self.layer1(x)  # 32x32\n",
    "        x = self.layer2(x)  # 16x16\n",
    "        x = self.layer3(x)  # 8x8\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.quant(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.quant_half(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(num_bits, quant_method, num_epochs):\n",
    "    if num_bits == 8:\n",
    "        bit = FloatingPoint(exp=5, man=2)\n",
    "    else:\n",
    "        bit = FloatingPoint(exp=6, man=9)\n",
    "    if quant_method == 'stochastic':\n",
    "        method = 'stochastic'\n",
    "    else:\n",
    "        method = 'nearest'\n",
    "    weight_quant = quantizer(forward_number=bit,\n",
    "                        forward_rounding=method)\n",
    "    grad_quant = quantizer(forward_number=bit,\n",
    "                            forward_rounding=method)\n",
    "    momentum_quant = quantizer(forward_number=bit,\n",
    "                            forward_rounding=method)\n",
    "    acc_quant = quantizer(forward_number=bit,\n",
    "                            forward_rounding=method)\n",
    "\n",
    "    # define a lambda function so that the Quantizer module can be duplicated easily\n",
    "    act_error_quant = lambda : Quantizer(forward_number=bit, backward_number=bit,\n",
    "                            forward_rounding=\"nearest\", backward_rounding=\"nearest\")\n",
    "    \n",
    "    model = PreResNet(act_error_quant, method=method)\n",
    "    model = model.to(device=device)\n",
    "    optimizer = SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
    "    optimizer = OptimLP(optimizer,\n",
    "                    weight_quant=weight_quant,\n",
    "                    grad_quant=grad_quant,\n",
    "                    momentum_quant=momentum_quant,\n",
    "                    acc_quant=acc_quant,\n",
    "                    grad_scaling=1/1000 # do loss scaling\n",
    "                    )\n",
    "    for epoch in range(num_epochs):\n",
    "        train_res = run_epoch(loaders['train'], model, F.cross_entropy,\n",
    "                                    optimizer=optimizer, phase=\"train\")\n",
    "        test_res = run_epoch(loaders['test'], model, F.cross_entropy,\n",
    "                                    optimizer=optimizer, phase=\"eval\")\n",
    "    return train_res, test_res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define two floating point formats\n",
    "# bit_8 = FloatingPoint(exp=5, man=2)\n",
    "# bit_16 = FloatingPoint(exp=6, man=9)\n",
    "\n",
    "# # define quantization functions\n",
    "# weight_quant = quantizer(forward_number=bit_8,\n",
    "#                         forward_rounding=\"nearest\")\n",
    "# grad_quant = quantizer(forward_number=bit_8,\n",
    "#                         forward_rounding=\"nearest\")\n",
    "# momentum_quant = quantizer(forward_number=bit_8,\n",
    "#                         forward_rounding=\"nearest\")\n",
    "# acc_quant = quantizer(forward_number=bit_8,\n",
    "#                         forward_rounding=\"nearest\")\n",
    "\n",
    "# # define a lambda function so that the Quantizer module can be duplicated easily\n",
    "# act_error_quant = lambda : Quantizer(forward_number=bit_8, backward_number=bit_8,\n",
    "#                         forward_rounding=\"nearest\", backward_rounding=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PreResNet(act_error_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cpu' # change device to 'cpu' if you want to run this example on cpu\n",
    "# model = model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
    "# optimizer = OptimLP(optimizer,\n",
    "#                     weight_quant=weight_quant,\n",
    "#                     grad_quant=grad_quant,\n",
    "#                     momentum_quant=momentum_quant,\n",
    "#                     acc_quant=acc_quant,\n",
    "#                     grad_scaling=1/1000 # do loss scaling\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, model, criterion, optimizer=None, phase=\"train\"):\n",
    "    assert phase in [\"train\", \"eval\"], \"invalid running phase\"\n",
    "    loss_sum = 0.0\n",
    "    correct = 0.0\n",
    "\n",
    "    if phase==\"train\": model.train()\n",
    "    elif phase==\"eval\": model.eval()\n",
    "\n",
    "    ttl = 0\n",
    "    with torch.autograd.set_grad_enabled(phase==\"train\"):\n",
    "        for i, (input, target) in tqdm(enumerate(loader), total=len(loader)):\n",
    "            input = input.to(device=device)\n",
    "            target = target.to(device=device)\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            loss_sum += loss.cpu().item() * input.size(0)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            ttl += input.size()[0]\n",
    "\n",
    "            if phase==\"train\":\n",
    "                loss = loss * 1000 # do loss scaling\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    correct = correct.cpu().item()\n",
    "    return {\n",
    "        'loss': loss_sum / float(ttl),\n",
    "        'accuracy': correct / float(ttl) * 100.0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [17:02<00:00,  2.61s/it]\n",
      "100%|██████████| 79/79 [01:32<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(1):\n",
    "#     train_res = run_epoch(loaders['train'], model, F.cross_entropy,\n",
    "#                                 optimizer=optimizer, phase=\"train\")\n",
    "#     test_res = run_epoch(loaders['test'], model, F.cross_entropy,\n",
    "#                                 optimizer=optimizer, phase=\"eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bit_16 with stochastic rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 1.6531662404632568, 'accuracy': 37.754}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {'loss': 1.6531662404632568, 'accuracy': 37.754}\n",
    "# train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 1.6229413803100585, 'accuracy': 42.970000000000006}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {'loss': 1.6229413803100585, 'accuracy': 42.970000000000006}\n",
    "# test_res "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bit_16 with nearest rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 2.148055100631714, 'accuracy': 21.534}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#{'loss': 2.148055100631714, 'accuracy': 21.534}\n",
    "# train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 2.069708095550537, 'accuracy': 24.02}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#{'loss': 2.069708095550537, 'accuracy': 24.02}\n",
    "# test_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bit_8 with stochastic rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 1.6669690743637084, 'accuracy': 36.978}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {'loss': 1.6669690743637084, 'accuracy': 36.978}\n",
    "# train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 1.4921433233261108, 'accuracy': 46.2}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {'loss': 1.4921433233261108, 'accuracy': 46.2}\n",
    "# test_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bit_8 with nearest rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 2.1043335503387453, 'accuracy': 22.864}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {'loss': 2.1043335503387453, 'accuracy': 22.864}\n",
    "# train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 2.0364970796585085, 'accuracy': 25.130000000000003}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {'loss': 2.0364970796585085, 'accuracy': 25.130000000000003}\n",
    "# test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_8_loss = []\n",
    "stochastic_8_acc = []\n",
    "stochastic_16_loss = []\n",
    "stochastic_16_acc = []\n",
    "nearest_8_loss = []\n",
    "nearest_8_acc = []\n",
    "nearesst_16_loss = []\n",
    "nearesst_16_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [16:08<00:00,  2.48s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.13s/it]\n",
      "100%|██████████| 391/391 [16:04<00:00,  2.47s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.14s/it]\n",
      "100%|██████████| 391/391 [16:04<00:00,  2.47s/it]\n",
      "100%|██████████| 79/79 [01:30<00:00,  1.14s/it]\n",
      "100%|██████████| 391/391 [16:00<00:00,  2.46s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.13s/it]\n",
      "100%|██████████| 391/391 [15:56<00:00,  2.45s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.14s/it]\n",
      "100%|██████████| 391/391 [15:54<00:00,  2.44s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.13s/it]\n",
      "100%|██████████| 391/391 [15:57<00:00,  2.45s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.14s/it]\n",
      "100%|██████████| 391/391 [15:54<00:00,  2.44s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.13s/it]\n",
      "100%|██████████| 391/391 [15:57<00:00,  2.45s/it]\n",
      "100%|██████████| 79/79 [01:30<00:00,  1.14s/it]\n",
      "100%|██████████| 391/391 [15:49<00:00,  2.43s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.14s/it]\n",
      "100%|██████████| 391/391 [15:31<00:00,  2.38s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.14s/it]\n",
      "100%|██████████| 391/391 [15:36<00:00,  2.40s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.14s/it]\n",
      "100%|██████████| 391/391 [15:37<00:00,  2.40s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.14s/it]\n",
      "100%|██████████| 391/391 [15:37<00:00,  2.40s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.14s/it]\n",
      "100%|██████████| 391/391 [15:37<00:00,  2.40s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.14s/it]\n",
      "100%|██████████| 391/391 [15:38<00:00,  2.40s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.13s/it]\n",
      "100%|██████████| 391/391 [15:36<00:00,  2.40s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.13s/it]\n",
      "100%|██████████| 391/391 [15:35<00:00,  2.39s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.13s/it]\n",
      "100%|██████████| 391/391 [15:35<00:00,  2.39s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.13s/it]\n",
      "100%|██████████| 391/391 [15:35<00:00,  2.39s/it]\n",
      "100%|██████████| 79/79 [01:29<00:00,  1.13s/it]\n",
      "100%|██████████| 391/391 [15:27<00:00,  2.37s/it]\n",
      "100%|██████████| 79/79 [01:27<00:00,  1.11s/it]\n",
      "100%|██████████| 391/391 [15:19<00:00,  2.35s/it]\n",
      "100%|██████████| 79/79 [01:27<00:00,  1.11s/it]\n",
      "100%|██████████| 391/391 [15:24<00:00,  2.36s/it]\n",
      "100%|██████████| 79/79 [01:27<00:00,  1.10s/it]\n",
      "100%|██████████| 391/391 [15:25<00:00,  2.37s/it]\n",
      "100%|██████████| 79/79 [01:27<00:00,  1.11s/it]\n",
      "100%|██████████| 391/391 [15:23<00:00,  2.36s/it]\n",
      "100%|██████████| 79/79 [01:27<00:00,  1.11s/it]\n",
      "100%|██████████| 391/391 [15:23<00:00,  2.36s/it]\n",
      "100%|██████████| 79/79 [01:27<00:00,  1.10s/it]\n",
      "100%|██████████| 391/391 [15:24<00:00,  2.36s/it]\n",
      "100%|██████████| 79/79 [01:27<00:00,  1.11s/it]\n",
      "100%|██████████| 391/391 [15:23<00:00,  2.36s/it]\n",
      "100%|██████████| 79/79 [01:27<00:00,  1.10s/it]\n",
      "100%|██████████| 391/391 [15:23<00:00,  2.36s/it]\n",
      "100%|██████████| 79/79 [01:27<00:00,  1.10s/it]\n",
      "100%|██████████| 391/391 [15:23<00:00,  2.36s/it]\n",
      "100%|██████████| 79/79 [01:27<00:00,  1.10s/it]\n",
      "100%|██████████| 391/391 [15:00<00:00,  2.30s/it]\n",
      "100%|██████████| 79/79 [01:27<00:00,  1.10s/it]\n",
      "100%|██████████| 391/391 [15:02<00:00,  2.31s/it]\n",
      "100%|██████████| 79/79 [01:27<00:00,  1.11s/it]\n",
      "100%|██████████| 391/391 [15:05<00:00,  2.32s/it]\n",
      "100%|██████████| 79/79 [01:27<00:00,  1.11s/it]\n",
      "100%|██████████| 391/391 [15:08<00:00,  2.32s/it]\n",
      "100%|██████████| 79/79 [01:27<00:00,  1.11s/it]\n",
      "100%|██████████| 391/391 [16:01<00:00,  2.46s/it]\n",
      "100%|██████████| 79/79 [01:35<00:00,  1.21s/it]\n",
      "100%|██████████| 391/391 [16:37<00:00,  2.55s/it]\n",
      "100%|██████████| 79/79 [01:36<00:00,  1.23s/it]\n",
      "100%|██████████| 391/391 [15:08<00:00,  2.32s/it]\n",
      "100%|██████████| 79/79 [01:35<00:00,  1.21s/it]\n",
      "100%|██████████| 391/391 [16:24<00:00,  2.52s/it]\n",
      "100%|██████████| 79/79 [01:34<00:00,  1.20s/it]\n",
      "100%|██████████| 391/391 [16:32<00:00,  2.54s/it]\n",
      "100%|██████████| 79/79 [01:36<00:00,  1.22s/it]\n",
      "100%|██████████| 391/391 [16:55<00:00,  2.60s/it]\n",
      "100%|██████████| 79/79 [01:36<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "stochastic_8 = run_model(8, 'stochastic', 10)\n",
    "stochastic_16 = run_model(16, 'stochastic', 10)\n",
    "nearest_8 = run_model(8, 'nearest', 10)\n",
    "nearest_16 = run_model(16, 'nearest', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'loss': 0.6599296322631836, 'accuracy': 77.13600000000001},\n",
       " {'loss': 0.8267228102684021, 'accuracy': 74.2})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stochastic_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'loss': 1.979677771987915, 'accuracy': 26.894000000000002},\n",
       " {'loss': 1.9590867292404175, 'accuracy': 26.279999999999998})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'loss': 0.5586580847358703, 'accuracy': 80.66600000000001},\n",
       " {'loss': 0.6888263773918152, 'accuracy': 77.56})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stochastic_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'loss': 0.543385420217514, 'accuracy': 81.294},\n",
       " {'loss': 0.785239832019806, 'accuracy': 75.13})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
